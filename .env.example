# This is an example .env file.
# Copy this file to .env and fill in your actual API keys and configurations.
# Do NOT commit your actual .env file with sensitive keys to version control.

# --- AI Provider Selection ---
# Choose your AI provider: "anthropic" or "lm_studio"
AI_PROVIDER="anthropic"

# --- Anthropic API Configuration (if AI_PROVIDER="anthropic") ---
ANTHROPIC_API_KEY="YOUR_ANTHROPIC_API_KEY_HERE"
# Model examples: "claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"
DEFAULT_ANTHROPIC_MODEL="claude-3-sonnet-20240229"

# --- LM Studio Configuration (if AI_PROVIDER="lm_studio") ---
# Typically http://localhost:1234/v1 if LM Studio server is running locally
LM_STUDIO_API_BASE="http://localhost:1234/v1"
# The model identifier as served by LM Studio (e.g., "local-model", or a specific model path if supported by your LM Studio setup)
# Often, you load the model in LM Studio UI, and the server uses the loaded model.
# This can be left as a placeholder or a specific name if your LM Studio server requires it.
LM_STUDIO_MODEL_NAME="loaded-model-in-lm-studio" 

# --- General AI Configuration ---
# Maximum number of tokens the AI is allowed to generate in a single response.
MAX_AI_OUTPUT_TOKENS=4096

# Context summarization settings (estimated token counts)
CONTEXT_TOKEN_HARD_LIMIT=180000 
CONTEXT_TOKEN_SOFT_LIMIT=150000 
SUMMARIZED_HISTORY_TARGET_TOKENS=20000 
MIN_MESSAGES_TO_KEEP_BEFORE_SUMMARY=6 

# --- Search API Keys (Optional) ---
GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY_HERE"
GOOGLE_CSE_ID="YOUR_GOOGLE_CSE_ID_HERE"
TAVILY_API_KEY="YOUR_TAVILY_API_KEY_HERE"
BRAVE_SEARCH_API_KEY="YOUR_BRAVE_SEARCH_API_KEY_HERE"

# --- Tool Configuration ---
DEFAULT_COMMAND_TIMEOUT=300 
REQUIRE_COMMAND_CONFIRMATION="True"

# --- Logging Configuration ---
LOG_FILE_PATH="logs/kali_ai_tool.log"
LOG_LEVEL_FILE="DEBUG"    
LOG_LEVEL_CONSOLE="WARNING" 
